{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 07 - Convolutional Neural Networks\n",
    "\n",
    "In this lesson, we will learn about convolutional neural networks (CNNs). In particular, we will focus on so-called 1D CNNs that are used for time series analysis or natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:39.401816Z",
     "start_time": "2020-01-14T12:17:34.548056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\ml\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, MinMaxScaler, RobustScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling1D, GlobalMaxPooling1D, Activation, Input, LSTM, GRU, Dense, Dropout, Flatten, Embedding, SpatialDropout1D, Bidirectional, CuDNNGRU\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Flatten, concatenate\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, CSVLogger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from keras import backend as K\n",
    "\n",
    "# This part required only for GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "seed = 10293239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:39.414805Z",
     "start_time": "2020-01-14T12:17:39.403810Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() * 0.8\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and basic info\n",
    "\n",
    "Let's load the same dataset as in Lesson 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:39.593720Z",
     "start_time": "2020-01-14T12:17:39.416802Z"
    }
   },
   "outputs": [],
   "source": [
    "bugs = pd.read_csv('./data/bugs_train.csv', parse_dates=['Opened', 'Changed'], index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:39.614710Z",
     "start_time": "2020-01-14T12:17:39.595717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component</th>\n",
       "      <th>Assignee</th>\n",
       "      <th>Status</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Opened</th>\n",
       "      <th>Changed</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Debug</td>\n",
       "      <td>eclipse</td>\n",
       "      <td>VERIFIED</td>\n",
       "      <td>Icons needed for actions (1GI5UXW)</td>\n",
       "      <td>2001-10-10 22:14:41</td>\n",
       "      <td>2001-10-18 11:51:14</td>\n",
       "      <td>P1</td>\n",
       "      <td>enhancement</td>\n",
       "      <td>FIXED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Debug</td>\n",
       "      <td>darin.eclipse</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>README: Hit count not reset (1GET20Y)</td>\n",
       "      <td>2001-10-10 22:14:42</td>\n",
       "      <td>2001-11-28 13:42:46</td>\n",
       "      <td>P3</td>\n",
       "      <td>normal</td>\n",
       "      <td>WORKSFORME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Debug</td>\n",
       "      <td>darin.eclipse</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Use styled text in console (1G9S1YF)</td>\n",
       "      <td>2001-10-10 22:14:43</td>\n",
       "      <td>2002-06-26 11:32:05</td>\n",
       "      <td>P3</td>\n",
       "      <td>normal</td>\n",
       "      <td>WONTFIX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Debug</td>\n",
       "      <td>eclipse</td>\n",
       "      <td>VERIFIED</td>\n",
       "      <td>StringBuffer representation (1GE3BFA)</td>\n",
       "      <td>2001-10-10 22:14:44</td>\n",
       "      <td>2014-12-02 06:37:26</td>\n",
       "      <td>P3</td>\n",
       "      <td>normal</td>\n",
       "      <td>FIXED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Component       Assignee    Status                                Summary  \\\n",
       "0     Debug        eclipse  VERIFIED     Icons needed for actions (1GI5UXW)   \n",
       "1     Debug  darin.eclipse  RESOLVED  README: Hit count not reset (1GET20Y)   \n",
       "2     Debug  darin.eclipse    CLOSED   Use styled text in console (1G9S1YF)   \n",
       "3     Debug        eclipse  VERIFIED  StringBuffer representation (1GE3BFA)   \n",
       "\n",
       "               Opened             Changed Priority     Severity  Resolution  \n",
       "0 2001-10-10 22:14:41 2001-10-18 11:51:14       P1  enhancement       FIXED  \n",
       "1 2001-10-10 22:14:42 2001-11-28 13:42:46       P3       normal  WORKSFORME  \n",
       "2 2001-10-10 22:14:43 2002-06-26 11:32:05       P3       normal     WONTFIX  \n",
       "3 2001-10-10 22:14:44 2014-12-02 06:37:26       P3       normal       FIXED  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugs.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classification task (the problem to solve)\n",
    "\n",
    "Our task remains the same for this lesson - we would be to predict what will be the resolution of the defect report (y) based on the description of a defect (X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (features)\n",
    "\n",
    "Let's quickly replicate processing of the Component and Severity features, as well as converting the decision class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:43.848826Z",
     "start_time": "2020-01-14T12:17:39.616706Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will make a copy of the main data\n",
    "bugs_small = bugs[[\"Assignee\", \"Component\", \"Severity\", \"Status\", \"Priority\", \"Opened\", \"Changed\", \"Summary\", \"Resolution\"]]\n",
    "\n",
    "# Component\n",
    "bugs_small = pd.get_dummies(bugs_small, columns=['Component'], prefix=\"Component\")\n",
    "\n",
    "# Severity\n",
    "bugs_small['Severity'] = bugs_small['Severity'].map(\n",
    "    {'enhancement':0, 'trivial':1, 'minor':2, 'normal':3, 'major':4, 'critical':5, 'blocker':6})\n",
    "\n",
    "# Status\n",
    "bugs_small['Status'] = bugs_small['Status'].map(\n",
    "    {'VERIFIED':0, 'RESOLVED':1, 'CLOSED':2})\n",
    "\n",
    "# Priority\n",
    "bugs_small['Priority'] = bugs_small['Priority'].map(\n",
    "    {'P1':1, 'P2':2, 'P3':3, 'P4':4, 'P5':5})\n",
    "\n",
    "\n",
    "y = bugs_small['Resolution']\n",
    "X = bugs_small.drop(['Resolution'], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "# Days\n",
    "X['Days'] = X.apply(lambda x: (x.Changed - x.Opened).days, axis=1)\n",
    "X.drop([\"Changed\", \"Opened\"], inplace=True, axis=1)\n",
    "\n",
    "# Assignee\n",
    "inbox = [1 if x.endswith('-inbox') else 0 for x in bugs_small['Assignee']]\n",
    "X.insert(loc=0, column='Assignee_Inbox', value=pd.Series(inbox))\n",
    "    \n",
    "eclipse = [1 if x.endswith('eclipse') else 0 for x in bugs_small['Assignee']]\n",
    "X.insert(loc=0, column='Assignee_Eclipse', value=pd.Series(eclipse))\n",
    "X.drop([\"Assignee\"], inplace=True, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:43.879803Z",
     "start_time": "2020-01-14T12:17:43.852816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assignee_Eclipse</th>\n",
       "      <th>Assignee_Inbox</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Status</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Component_APT</th>\n",
       "      <th>Component_Core</th>\n",
       "      <th>Component_Debug</th>\n",
       "      <th>Component_Doc</th>\n",
       "      <th>Component_Text</th>\n",
       "      <th>Component_UI</th>\n",
       "      <th>Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Icons needed for actions (1GI5UXW)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>README: Hit count not reset (1GET20Y)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Use styled text in console (1G9S1YF)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>StringBuffer representation (1GE3BFA)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Assignee_Eclipse  Assignee_Inbox  Severity  Status  Priority  \\\n",
       "0                 1               0         0       0         1   \n",
       "1                 1               0         3       1         3   \n",
       "2                 1               0         3       2         3   \n",
       "3                 1               0         3       0         3   \n",
       "\n",
       "                                 Summary  Component_APT  Component_Core  \\\n",
       "0     Icons needed for actions (1GI5UXW)              0               0   \n",
       "1  README: Hit count not reset (1GET20Y)              0               0   \n",
       "2   Use styled text in console (1G9S1YF)              0               0   \n",
       "3  StringBuffer representation (1GE3BFA)              0               0   \n",
       "\n",
       "   Component_Debug  Component_Doc  Component_Text  Component_UI  Days  \n",
       "0                1              0               0             0     7  \n",
       "1                1              0               0             0    48  \n",
       "2                1              0               0             0   258  \n",
       "3                1              0               0             0  4800  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:43.892796Z",
     "start_time": "2020-01-14T12:17:43.881800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary = np.array([1 if x == \"FIXED\" else 0 for x in y])\n",
    "y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:43.937775Z",
     "start_time": "2020-01-14T12:17:43.894794Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.33, random_state=seed, stratify=y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text input from defect-report summaries\n",
    "\n",
    "We have to convert the text in the summary column into a sequence of numbers. Each number will correspond to the index of the word it represent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:43.953766Z",
     "start_time": "2020-01-14T12:17:43.939774Z"
    }
   },
   "outputs": [],
   "source": [
    "train_summaries = [str(x) for x in X_train['Summary'].tolist()]\n",
    "test_summaries = [str(x) for x in X_test['Summary'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:43.959769Z",
     "start_time": "2020-01-14T12:17:43.954766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Code Formatter exchange several blank lines  w/ one',\n",
       " 'TVT3.0:  HTML files missing DOCTYPE tag']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_summaries[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T15:33:32.147866Z",
     "start_time": "2020-01-13T15:33:32.132873Z"
    }
   },
   "source": [
    "### Tokenizing the text\n",
    "\n",
    "In the first step, we have to tokenize the text. You can use tokenizers from the NLP libraries like NLTK or Spicy. However, to simpify we will use here a basic tokenizer offered by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.589741Z",
     "start_time": "2020-01-14T12:17:43.961763Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_summaries = [text_to_word_sequence(x, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                   lower=True, split=' ') for x in train_summaries]\n",
    "tokenized_test_summaries = [text_to_word_sequence(x, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                   lower=True, split=' ') for x in test_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.594739Z",
     "start_time": "2020-01-14T12:17:44.590740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code', 'formatter', 'exchange', 'several', 'blank', 'lines', 'w', 'one']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.661705Z",
     "start_time": "2020-01-14T12:17:44.595738Z"
    }
   },
   "outputs": [],
   "source": [
    "# we have to flatten the tokenized_train_summaries to a single list to build the vocabulary\n",
    "tokenized_train_summaries_flat = list(itertools.chain(*tokenized_train_summaries))\n",
    "\n",
    "counter = Counter(tokenized_train_summaries_flat)\n",
    "\n",
    "# how many most common words will we take?\n",
    "NUM_WORDS = 6000\n",
    "\n",
    "# we construct a vocabulary\n",
    "if NUM_WORDS is not None:\n",
    "    vocab = [x for x,y in counter.most_common(NUM_WORDS)]\n",
    "else:\n",
    "    vocab = counter.keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert each text to a sequence of word indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.668703Z",
     "start_time": "2020-01-14T12:17:44.663704Z"
    }
   },
   "outputs": [],
   "source": [
    "# we reserve 0 - padding and 1 a token that is not in the vocabulary\n",
    "# if the embedding is frozen it would be enough to have only zero preserved for padding \n",
    "def tokens2index(tokens, vocab):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            result.append(vocab.index(token)+2)\n",
    "        else:\n",
    "            result.append(1) ## OOV\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.674700Z",
     "start_time": "2020-01-14T12:17:44.669703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 47, 3107, 1069, 640, 282, 923, 245]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2index(tokenized_train_summaries[0], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences so they have equal length\n",
    "\n",
    "We need to make all the sequences of equal length. Let's see what is the distribution of sequence lengths and choose the length that will be sufficient for most of the cases. Of course, the shorter the length the less parameters of the model, so we have to choose wisely to balance between the model complexity and ability to cover the caes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.809636Z",
     "start_time": "2020-01-14T12:17:44.675700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value for 90% is 13.0, 98% is 16.0, 99% is 18.0, and 100% is 42.0\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(x) for x in tokenized_train_summaries]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "# plot the cumulative histogram\n",
    "n_bins = 5\n",
    "n, bins, patches = ax.hist(lengths, n_bins, density=True, histtype='step',\n",
    "                           cumulative=True, label='Empirical')\n",
    "\n",
    "plt.xlim((0,40))\n",
    "plt.show()\n",
    "print(\"The value for 90% is {}, 98% is {}, 99% is {}, and 100% is {}\".format(*np.percentile(lengths, [90, 98, 99, 100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that  if we choose 16 it should cover 99% of cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.814634Z",
     "start_time": "2020-01-14T12:17:44.810636Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:44.824628Z",
     "start_time": "2020-01-14T12:17:44.816633Z"
    }
   },
   "outputs": [],
   "source": [
    "def texts2index_padded(texts, vocab, seq_length):\n",
    "    texts_ids = [tokens2index(tokens, vocab) for tokens in texts]\n",
    "    return pad_sequences(texts_ids, maxlen=seq_length, \n",
    "                          dtype='int32', padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.270892Z",
     "start_time": "2020-01-14T12:17:44.826626Z"
    }
   },
   "outputs": [],
   "source": [
    "padded_train_summaries_ids = texts2index_padded(tokenized_train_summaries, vocab, seq_length=MAX_SEQUENCE_LENGTH)\n",
    "padded_test_summaries_ids = texts2index_padded(tokenized_test_summaries, vocab, seq_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.277889Z",
     "start_time": "2020-01-14T12:17:52.272892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15,   47, 3107, 1069,  640,  282,  923,  245,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0],\n",
       "       [1230,  118,  599,   94,   59, 4327,  293,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_summaries_ids[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the layers\n",
    "\n",
    "We will now go through different type of layers before we will build a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "We have transformed text to vectors of numbers. That's great. But could we use it train a model? Well, no. Indices of words would be treated as numbers, i.e., treated as a ratio variable. This is meaningless.\n",
    "\n",
    "Therefore, we need a vector representation of each word that consists of \"true\" numbers. We create a layer that will work as a mapping between a word indices and such vectors. Then, these vectors can be determined during the process of training the network or trained separately and \"imported\" to the model (we call it transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.284886Z",
     "start_time": "2020-01-14T12:17:52.279889Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will be an input to our network - for each defect - a list of word indices\n",
    "text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding matrix have two dimensions. The input dimension will be the number of words in our vocabulary and the output variable will be the length of embedding vector. The output of the layer will be of size sequence length x embedding vector length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.290883Z",
     "start_time": "2020-01-14T12:17:52.286885Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.306876Z",
     "start_time": "2020-01-14T12:17:52.292882Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=INPUT_DIM,\n",
    "                 output_dim=OUTPUT_DIM,\n",
    "                 mask_zero=False,\n",
    "                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                 trainable=True)\n",
    "embedded_sequences = embedding_layer(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.355852Z",
     "start_time": "2020-01-14T12:17:52.308876Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model([text_input] , embedded_sequences) \n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.362848Z",
     "start_time": "2020-01-14T12:17:52.357852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 16, 10)            60000     \n",
      "=================================================================\n",
      "Total params: 60,000\n",
      "Trainable params: 60,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.367847Z",
     "start_time": "2020-01-14T12:17:52.363848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Code Formatter exchange several blank lines  w/ one'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we embed it (we have to reshape it so it is a two dimensional array 1x16), we obtain what we expected. Of course, our current word embeddings is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.423820Z",
     "start_time": "2020-01-14T12:17:52.368847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_train_summaries_ids[0].reshape(1,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.431816Z",
     "start_time": "2020-01-14T12:17:52.424819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.03841952, -0.00128288, -0.03919214,  0.03979334,\n",
       "          0.04737847,  0.04214337,  0.00811468,  0.03492378,\n",
       "          0.03149137,  0.03812524],\n",
       "        [-0.01242846,  0.01209965,  0.04128165,  0.02369097,\n",
       "          0.01772586,  0.04846778,  0.0133849 , -0.03904023,\n",
       "         -0.02213336,  0.03322763],\n",
       "        [ 0.03121069, -0.00166617,  0.0264242 , -0.03519494,\n",
       "          0.03194935,  0.01872606,  0.0329368 , -0.01756637,\n",
       "          0.02249714,  0.01990855],\n",
       "        [-0.02714703,  0.02641514,  0.03698857,  0.03335297,\n",
       "         -0.01993429, -0.04864558,  0.00081931, -0.01338198,\n",
       "          0.03055773, -0.02230312],\n",
       "        [ 0.00951511,  0.03685622,  0.0200111 ,  0.03992688,\n",
       "         -0.03834791, -0.03242838, -0.02334448,  0.04851443,\n",
       "          0.01146143,  0.02392732],\n",
       "        [ 0.01439441,  0.00146855, -0.01043812,  0.02573225,\n",
       "         -0.03110491,  0.02358406,  0.0017427 ,  0.04552073,\n",
       "         -0.02222648,  0.0140429 ],\n",
       "        [-0.00382056,  0.00107156, -0.00688099, -0.03141645,\n",
       "          0.04177609,  0.0239484 ,  0.00113002,  0.01077925,\n",
       "         -0.03911258,  0.00810418],\n",
       "        [ 0.04957477, -0.0261035 , -0.00190517, -0.00326188,\n",
       "         -0.01283009, -0.04421217,  0.01271367,  0.01107635,\n",
       "          0.02828452, -0.03860583],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788],\n",
       "        [-0.00623115,  0.02564069,  0.02781813, -0.01374112,\n",
       "          0.02824322,  0.0039084 , -0.00360329,  0.04273113,\n",
       "          0.00772823,  0.00590788]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_train_summaries_ids[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "This is a heart of any convolutional neural network. Here, we will use a layer called Conv1D. The 1D could be confusing because in fact it is accepts two-dimensional inputs (see our embedding dimension -> 16x10). However, the second dimension is not really a dimension in that sense (it is called depth or number of channels). It is maybe easier to understand for the Conv2D layer which accepts three-dimensional input. This one is usually used for image processing. An image has width and lenght - these are its dimensions, and number of channels - yes, this is the magical third dimension that is treated differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simpify our input for a while and assume it has dimension of 4x3 (it will be more compact). So, we have a sequence of 4 elements and each is represented by a vector of 3 numbers.\n",
    "\n",
    "Convolutional layer trains so-called filters. Each filter \"convolves\" (moves) through the input and the output is produced by multiplying the corresponding values and weights of the filter, summing them and adding bias (and then passed to the activation function).\n",
    "\n",
    "\n",
    "Convolutional layer has multiple parameters but the most important are:\n",
    "* filters - how many filters to train\n",
    "* kernel_size - what will be the \"scanning\" area of each filter\n",
    "* stride - how much the filter moves \n",
    "\n",
    "A nice visualization of how convolution works could be found here - http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.559757Z",
     "start_time": "2020-01-14T12:17:52.432816Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_input = Input(shape=(4, 3), dtype='float32', name=\"input\")\n",
    "\n",
    "output = Conv1D(filters=2, kernel_size=2,  strides=1,\n",
    "                kernel_initializer='ones',\n",
    "                bias_initializer='ones')(conv_input)\n",
    "\n",
    "model = Model([conv_input] , output) \n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.564755Z",
     "start_time": "2020-01-14T12:17:52.560754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3, 2)              14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a single convolutional layer, which consists of two filters. Each filter has receptive fields of two elements and stride of one element. We also initialize all the filters and bias with 1 so we can see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our layer has 14 parameters. Can you tell why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our layer's output has the dimension of 3x2. Can you tell why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have your guess but let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the input. Look at it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:52.574749Z",
     "start_time": "2020-01-14T12:17:52.565754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[1,1,3], \n",
    "               [2,1,1], \n",
    "               [3,1,1],\n",
    "               [3,1,1]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.723961Z",
     "start_time": "2020-01-14T12:17:52.576748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10., 10.],\n",
       "        [10., 10.],\n",
       "        [11., 11.]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we see that it is really like this. But, let's think how it happens. \n",
    "\n",
    "Let's start from the number of parameters. Each filter has a receptive field of 2 elements and each element is represented by a vector of 3 numbers. So each filter has the size of 3x2. This gives us 12 parameters + bias for each of the filters. So, this results in 14 parameters.\n",
    "\n",
    "Let's now see how the first row of the output is produced => [10, 10].\n",
    "\n",
    "[[1,1,3], [2,1,1]] dot product [[1,1],[1,1],[1,1]] -> sum it \"channel-wise\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.731949Z",
     "start_time": "2020-01-14T12:17:53.725952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5],\n",
       "       [4, 4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.dot(np.array([[1,1,3], [2,1,1]]), np.array([[1,1],[1,1],[1,1]]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.739945Z",
     "start_time": "2020-01-14T12:17:53.732951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(res, axis=0) + np.array([1,1]) #bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the output from both filters for this \"patch\" is the same and equal to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to tell how the remaining two rows of the output are created (btw. we call the output - feature map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.747942Z",
     "start_time": "2020-01-14T12:17:53.741946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.dot(np.array([[2,1,1], [3,1,1]]), np.array([[1,1],[1,1],[1,1]]))\n",
    "res = np.sum(res, axis=0) + np.array([1,1]) #bias\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.753939Z",
     "start_time": "2020-01-14T12:17:53.748941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 11])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.dot(np.array([[3,1,1], [3,1,1]]), np.array([[1,1],[1,1],[1,1]]))\n",
    "res = np.sum(res, axis=0) + np.array([1,1]) #bias\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the receptive field of a filter is larger than the \"patch\" to analyze. It such a case we have different strategies to handle missing data, e.g., we pad them with zeros, or use the same values as on the border, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "Pooling is a technique of reducing dimensionality of the feature maps bo taking maximum, minimum, or average of certain patch. The hyperparameter pool_size defines the size of the patch. By default stride = pool_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.802917Z",
     "start_time": "2020-01-14T12:17:53.755938Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_input = Input(shape=(4, 3), dtype='float32', name=\"input\")\n",
    "\n",
    "output = MaxPooling1D(pool_size=2)(conv_input)\n",
    "\n",
    "model = Model([conv_input] , output) \n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.807913Z",
     "start_time": "2020-01-14T12:17:53.803915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 2, 3)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.814910Z",
     "start_time": "2020-01-14T12:17:53.808914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[1,1,3], \n",
    "               [2,1,1], \n",
    "               [3,1,1],\n",
    "               [3,1,1]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.873882Z",
     "start_time": "2020-01-14T12:17:53.816909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 1., 3.],\n",
       "        [3., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pool_size is equal to 2 and stride 2, we have two pairs to compare - rows 1,2 and 3,4. The first output is created as a max of row 1 and 2 row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.878881Z",
     "start_time": "2020-01-14T12:17:53.874881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array([[1,1,3], [2,1,1]]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Pooling \n",
    "\n",
    "Works similar as pooling, but operates on different axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.929857Z",
     "start_time": "2020-01-14T12:17:53.879881Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_input = Input(shape=(4, 3), dtype='float32', name=\"input\")\n",
    "\n",
    "output = GlobalMaxPooling1D()(conv_input)\n",
    "\n",
    "model = Model([conv_input] , output) \n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.934854Z",
     "start_time": "2020-01-14T12:17:53.930856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.941849Z",
     "start_time": "2020-01-14T12:17:53.935853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[1,1,3], \n",
    "               [2,1,1], \n",
    "               [3,1,1],\n",
    "               [3,1,1]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:53.999821Z",
     "start_time": "2020-01-14T12:17:53.942850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 1., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a model using summary texts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:54.015819Z",
     "start_time": "2020-01-14T12:17:54.001822Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, vector_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size+2, vector_dim))\n",
    "    for i in range(vocab_size+2):\n",
    "        if i == 1:\n",
    "            embedding_matrix[i] = np.zeros(vector_dim)\n",
    "        elif i > 1:\n",
    "            embedding_matrix[i] = np.random.random(vector_dim)\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_model_global_pool(seq_len=None, vocab=None, embed_size=None):\n",
    "    \n",
    "    global seed\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    vocab_size = len(vocab)+2\n",
    "    \n",
    "    text_input = Input(shape=(seq_len,), dtype='int32', name=\"input\")\n",
    "    \n",
    "    embedding_matrix = create_embedding_matrix(vocab, embed_size)\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                 output_dim=embed_size,\n",
    "                 weights=[embedding_matrix],\n",
    "                 mask_zero=False,\n",
    "                 input_length=seq_len,\n",
    "                 trainable=True)\n",
    "    embedded_sequences = embedding_layer(text_input)\n",
    "    cnn = Dropout(0.1)(embedded_sequences)\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(embedded_sequences)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(cnn)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Dropout(0.3)(cnn)\n",
    "    cnn = GlobalMaxPooling1D()(cnn)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(cnn)\n",
    "    \n",
    "    model = Model([text_input] , output)\n",
    "    algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_dense(seq_len=None, vocab=None, embed_size=None):\n",
    "    \n",
    "    global seed\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    vocab_size = len(vocab)+2\n",
    "    \n",
    "    text_input = Input(shape=(seq_len,), dtype='int32', name=\"input\")\n",
    "    \n",
    "    embedding_matrix = create_embedding_matrix(vocab, embed_size)\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                 output_dim=embed_size,\n",
    "                 weights=[embedding_matrix],\n",
    "                 mask_zero=False,\n",
    "                 input_length=seq_len,\n",
    "                 trainable=True)\n",
    "    embedded_sequences = embedding_layer(text_input)\n",
    "    cnn = Dropout(0.1)(embedded_sequences)\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(embedded_sequences)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(cnn)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    cnn = Dense(10, activation=\"relu\")(cnn)\n",
    "    cnn = Dropout(0.2)(cnn)\n",
    "    \n",
    "    output = Dense(1, activation=\"sigmoid\")(cnn)\n",
    "    \n",
    "    model = Model([text_input] , output)\n",
    "    algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:54.230711Z",
     "start_time": "2020-01-14T12:17:54.017814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 16, 5)             30010     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 16)            176       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 16)             528       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 30,731\n",
      "Trainable params: 30,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model_global_pool(seq_len=MAX_SEQUENCE_LENGTH, vocab=vocab, embed_size=5).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:54.517574Z",
     "start_time": "2020-01-14T12:17:54.231711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 16, 5)             30010     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16, 16)            176       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8, 16)             528       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 31,375\n",
      "Trainable params: 31,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model_dense(seq_len=MAX_SEQUENCE_LENGTH, vocab=vocab, embed_size=5).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:17:54.525572Z",
     "start_time": "2020-01-14T12:17:54.519574Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "\n",
    "callbacks_list = [\n",
    "    ReduceLROnPlateau( \n",
    "        monitor='loss',\n",
    "        min_lr=0.001, \n",
    "        factor=0.5,\n",
    "        verbose=1,\n",
    "        patience=10) \n",
    "]\n",
    "\n",
    "pipeline = []\n",
    "pipeline.append(('classifier', KerasClassifier(build_fn=get_model_dense,  \n",
    "                                        epochs=50,\n",
    "                                        batch_size=128, \n",
    "                                        verbose=2, \n",
    "                                        callbacks=callbacks_list,\n",
    "                                        seq_len=MAX_SEQUENCE_LENGTH, \n",
    "                                        vocab=vocab, \n",
    "                                        embed_size=5))) \n",
    "\n",
    "model = Pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:19:22.895103Z",
     "start_time": "2020-01-14T12:17:54.526570Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 0.6931 - acc: 0.5064\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.6771 - acc: 0.5719\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.6370 - acc: 0.6436\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.6056 - acc: 0.6767\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.5761 - acc: 0.7029\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.5483 - acc: 0.7260\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.5196 - acc: 0.7443\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.4986 - acc: 0.7602\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.4705 - acc: 0.7733\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.4467 - acc: 0.7881\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.4281 - acc: 0.7975\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.4085 - acc: 0.8064\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.3882 - acc: 0.8169\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.3734 - acc: 0.8204\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.3597 - acc: 0.8278\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.3535 - acc: 0.8311\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.3308 - acc: 0.8429\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.3233 - acc: 0.8452\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.3159 - acc: 0.8488\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.3049 - acc: 0.8542\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.2957 - acc: 0.8593\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.3005 - acc: 0.8563\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.2951 - acc: 0.8594\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.2875 - acc: 0.8610\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.2774 - acc: 0.8656\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.2792 - acc: 0.8664\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.2748 - acc: 0.8674\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2672 - acc: 0.8713\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.2653 - acc: 0.8739\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.2529 - acc: 0.8804\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.2580 - acc: 0.8765\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.2540 - acc: 0.8793\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.2526 - acc: 0.8799\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.2525 - acc: 0.8791\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2502 - acc: 0.8793\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.2457 - acc: 0.8832\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.2388 - acc: 0.8851\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.2341 - acc: 0.8885\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.2383 - acc: 0.8875\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.2389 - acc: 0.8856\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.2311 - acc: 0.8903\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.2277 - acc: 0.8928\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.2265 - acc: 0.8926\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.2261 - acc: 0.8933\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.2334 - acc: 0.8879\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.2218 - acc: 0.8949\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.2370 - acc: 0.8868\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.2234 - acc: 0.8931\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2292 - acc: 0.8915\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.2212 - acc: 0.8946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('classifier', <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021356F98D68>)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train_summaries_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:19:23.164789Z",
     "start_time": "2020-01-14T12:19:22.898107Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(padded_test_summaries_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:19:23.182754Z",
     "start_time": "2020-01-14T12:19:23.166762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy = 0.569, Precision = 0.570, Recall = 0.569, F1-score = 0.567'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:19:23.199746Z",
     "start_time": "2020-01-14T12:19:23.183754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3675, 3602],\n",
       "       [2677, 4614]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embeddings\n",
    "\n",
    "In the previous model, the word embeddings were trained during the task. However, there exist pre-trained word embeddings that try to capture relationship between the words (see GloVe or Word2Vec for more information). Here we will try to use one of such emmbeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a GloVe model trained on Wikipedia+Gigaworld and, in particular, the version embedding words into 50d vectors. The original file is nearly 170MB, so we will use only it extract containing words appearing in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:19:23.415650Z",
     "start_time": "2020-01-14T12:19:23.201745Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.json', 'r') as fp:\n",
    "    embeddings_idx = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:23:29.290210Z",
     "start_time": "2020-01-14T12:23:29.285212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'to']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(embeddings_idx.keys())\n",
    "vocab[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract such a smaller embedding yourself, you could use the code below (don't run it, it won't work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:19:23.710513Z",
     "start_time": "2020-01-14T12:19:23.427638Z"
    }
   },
   "outputs": [],
   "source": [
    "# we have to flatten the tokenized_train_summaries to a single list to build the vocabulary\n",
    "tokenized_train_summaries_flat = list(itertools.chain(*tokenized_train_summaries))\n",
    "\n",
    "counter = Counter(tokenized_train_summaries_flat)\n",
    "vocab = counter.keys()\n",
    "    \n",
    "embeddings_index = {}\n",
    "with open(\"Path to GloVe\\\\glove.6B.50d.txt\", encoding=\"utf8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if i % 10000:\n",
    "            print(word)\n",
    "        if word in vocab:\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "            except:\n",
    "                pass\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "embeddings_index_save = {}\n",
    "for k,v in embeddings_index.items():\n",
    "    embeddings_index_save[k] = v.tolist()\n",
    "    \n",
    "with open('glove.6B.50d.json', 'w') as fp:\n",
    "    json.dump(embeddings_index_save, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to now encode our summary texts using indices from the new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:23:52.721869Z",
     "start_time": "2020-01-14T12:23:33.574278Z"
    }
   },
   "outputs": [],
   "source": [
    "padded_train_summaries_ids = texts2index_padded(tokenized_train_summaries, vocab, seq_length=MAX_SEQUENCE_LENGTH)\n",
    "padded_test_summaries_ids = texts2index_padded(tokenized_test_summaries, vocab, seq_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:23:54.869819Z",
     "start_time": "2020-01-14T12:23:54.851830Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix_pretrained(vocab, embeddings_idx, vector_dim):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size+2, vector_dim))\n",
    "    for i in range(vocab_size+2):\n",
    "        if i == 1:\n",
    "            embedding_matrix[i] = np.zeros(vector_dim)\n",
    "        elif i > 1:\n",
    "            embedding_matrix[i] = embeddings_idx[vocab[i-2]]\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_model_global_pool(seq_len=None, vocab=None, embeddings_idx=None, embed_size=50):\n",
    "    \n",
    "    global seed\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    vocab_size = len(vocab)+2\n",
    "    \n",
    "    text_input = Input(shape=(seq_len,), dtype='int32', name=\"input\")\n",
    "    \n",
    "    embedding_matrix = create_embedding_matrix_pretrained(vocab, embeddings_idx, embed_size)\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                 output_dim=embed_size,\n",
    "                 weights=[embedding_matrix],\n",
    "                 mask_zero=False,\n",
    "                 input_length=seq_len,\n",
    "                 trainable=False)\n",
    "    embedded_sequences = embedding_layer(text_input)\n",
    "\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(embedded_sequences)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(cnn)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Dropout(0.2)(cnn)\n",
    "    cnn = GlobalMaxPooling1D()(cnn)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(cnn)\n",
    "    \n",
    "    model = Model([text_input] , output)\n",
    "    algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_dense(seq_len=None, vocab=None, embeddings_idx=None, embed_size=50):\n",
    "    \n",
    "    global seed\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    vocab_size = len(vocab)+2\n",
    "    \n",
    "    text_input = Input(shape=(seq_len,), dtype='int32', name=\"input\")\n",
    "    \n",
    "    embedding_matrix = create_embedding_matrix_pretrained(vocab, embeddings_idx, embed_size)\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                 output_dim=embed_size,\n",
    "                 weights=[embedding_matrix],\n",
    "                 mask_zero=False,\n",
    "                 input_length=seq_len,\n",
    "                 trainable=False)\n",
    "    embedded_sequences = embedding_layer(text_input)\n",
    "\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(embedded_sequences)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                        padding='same', activation='relu')(cnn)\n",
    "    cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    cnn = Dense(10, activation=\"relu\")(cnn)\n",
    "    cnn = Dropout(0.2)(cnn)\n",
    "    \n",
    "    output = Dense(1, activation=\"sigmoid\")(cnn)\n",
    "    \n",
    "    model = Model([text_input] , output)\n",
    "    algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:23:57.342863Z",
     "start_time": "2020-01-14T12:23:56.986979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 16, 50)            293400    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16, 16)            1616      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 8, 16)             528       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 295,561\n",
      "Trainable params: 2,161\n",
      "Non-trainable params: 293,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model_global_pool(seq_len=MAX_SEQUENCE_LENGTH, vocab=vocab, embeddings_idx=embeddings_idx).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:23:59.825752Z",
     "start_time": "2020-01-14T12:23:59.483913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 16, 50)            293400    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 16, 16)            1616      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8, 16)             528       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 296,205\n",
      "Trainable params: 2,805\n",
      "Non-trainable params: 293,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model_dense(seq_len=MAX_SEQUENCE_LENGTH, vocab=vocab, embeddings_idx=embeddings_idx).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:24:02.162814Z",
     "start_time": "2020-01-14T12:24:02.155818Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "\n",
    "callbacks_list = [\n",
    "    ReduceLROnPlateau( \n",
    "        monitor='loss',\n",
    "        min_lr=0.001, \n",
    "        factor=0.5,\n",
    "        verbose=1,\n",
    "        patience=10) \n",
    "]\n",
    "\n",
    "pipeline = []\n",
    "pipeline.append(('classifier', KerasClassifier(build_fn=get_model_global_pool,  \n",
    "                                        epochs=50,\n",
    "                                        batch_size=64, \n",
    "                                        verbose=2, \n",
    "                                        callbacks=callbacks_list,\n",
    "                                        seq_len=MAX_SEQUENCE_LENGTH, \n",
    "                                        vocab=vocab, \n",
    "                                        embeddings_idx=embeddings_idx))) \n",
    "\n",
    "model = Pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:29.468093Z",
     "start_time": "2020-01-14T12:24:04.295325Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 0.6915 - acc: 0.5270\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.6824 - acc: 0.5573\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.6758 - acc: 0.5723\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.6716 - acc: 0.5799\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.6682 - acc: 0.5868\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.6641 - acc: 0.5964\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.6633 - acc: 0.5950\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.6597 - acc: 0.6033\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.6559 - acc: 0.6082\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.6549 - acc: 0.6074\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.6515 - acc: 0.6114\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.6486 - acc: 0.6167\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.6472 - acc: 0.6167\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.6462 - acc: 0.6200\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.6443 - acc: 0.6230\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.6429 - acc: 0.6240\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.6402 - acc: 0.6278\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.6393 - acc: 0.6264\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.6386 - acc: 0.6317\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.6370 - acc: 0.6333\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.6346 - acc: 0.6352\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.6374 - acc: 0.6311\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.6344 - acc: 0.6354\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.6330 - acc: 0.6374\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.6322 - acc: 0.6347\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.6320 - acc: 0.6355\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.6315 - acc: 0.6342\n",
      "Epoch 28/50\n",
      " - 3s - loss: 0.6309 - acc: 0.6371\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.6302 - acc: 0.6377\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.6282 - acc: 0.6385\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.6281 - acc: 0.6382\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.6277 - acc: 0.6391\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.6281 - acc: 0.6363\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.6263 - acc: 0.6419\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.6253 - acc: 0.6411\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.6246 - acc: 0.6419\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.6249 - acc: 0.6428\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.6265 - acc: 0.6419\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.6253 - acc: 0.6427\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.6240 - acc: 0.6419\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.6223 - acc: 0.6472\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.6231 - acc: 0.6447\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.6224 - acc: 0.6453\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.6216 - acc: 0.6458\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.6230 - acc: 0.6458\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.6209 - acc: 0.6489\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.6208 - acc: 0.6456\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.6220 - acc: 0.6442\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.6202 - acc: 0.6442\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.6185 - acc: 0.6494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('classifier', <keras.wrappers.scikit_learn.KerasClassifier object at 0x000002137AC34320>)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train_summaries_ids, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:32.063231Z",
     "start_time": "2020-01-14T12:26:31.629088Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(padded_test_summaries_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:34.256258Z",
     "start_time": "2020-01-14T12:26:34.238262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy = 0.575, Precision = 0.576, Recall = 0.575, F1-score = 0.573'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:36.475224Z",
     "start_time": "2020-01-14T12:26:36.453235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4598, 2679],\n",
       "       [3518, 3773]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining CNN and MLP\n",
    "\n",
    "We will create an ANN that has two inputs. The first one will be the tokenized summary text and will use CNN to process it and the second one will be a regular MLP for other feautres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start from preparing the input data for the second input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:38.835145Z",
     "start_time": "2020-01-14T12:26:38.634233Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "X_train.drop([\"Summary\"], inplace=True, axis=1)\n",
    "X_test.drop([\"Summary\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:40.974786Z",
     "start_time": "2020-01-14T12:26:40.955788Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:43.537502Z",
     "start_time": "2020-01-14T12:26:43.118987Z"
    }
   },
   "outputs": [],
   "source": [
    "global seed\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "\n",
    "vocab_size = len(vocab)+2\n",
    "\n",
    "## SUMMARY\n",
    "\n",
    "text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name=\"summary_input\")\n",
    "\n",
    "embedding_matrix = create_embedding_matrix_pretrained(vocab, embeddings_idx, 50)\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "             output_dim=50,\n",
    "             weights=[embedding_matrix],\n",
    "             mask_zero=False,\n",
    "             input_length=MAX_SEQUENCE_LENGTH,\n",
    "             trainable=False)\n",
    "embedded_sequences = embedding_layer(text_input)\n",
    "\n",
    "cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                    padding='same', activation='relu')(embedded_sequences)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "cnn = Conv1D(filters=16, kernel_size=2, strides=1, \n",
    "                    padding='same', activation='relu')(cnn)\n",
    "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
    "cnn = Flatten()(cnn)\n",
    "\n",
    "### OTHER FEATURES\n",
    "other_features_input = Input(shape=(X_train.shape[1],), dtype='float32', name=\"other_features_input\")\n",
    "dense = other_features_input\n",
    "\n",
    "### MERGE\n",
    "\n",
    "merged = concatenate([dense, cnn])\n",
    "merged = Dense(10, activation=\"relu\")(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "\n",
    "output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "model = Model([text_input, other_features_input] , output)\n",
    "algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:26:45.623657Z",
     "start_time": "2020-01-14T12:26:45.617658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "summary_input (InputLayer)      (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 16, 50)       293400      summary_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 16, 16)       1616        embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 8, 16)        0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 8, 16)        528         max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 4, 16)        0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "other_features_input (InputLaye (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 64)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 76)           0           other_features_input[0][0]       \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           770         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 10)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            11          dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 296,325\n",
      "Trainable params: 2,925\n",
      "Non-trainable params: 293,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:28:08.559024Z",
     "start_time": "2020-01-14T12:26:47.745486Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29575 samples, validate on 14568 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 0.4783 - acc: 0.7807 - val_loss: 0.4014 - val_acc: 0.8266\n",
      "Epoch 2/20\n",
      " - 4s - loss: 0.4044 - acc: 0.8251 - val_loss: 0.3894 - val_acc: 0.8270\n",
      "Epoch 3/20\n",
      " - 4s - loss: 0.3945 - acc: 0.8264 - val_loss: 0.3835 - val_acc: 0.8289\n",
      "Epoch 4/20\n",
      " - 4s - loss: 0.3915 - acc: 0.8266 - val_loss: 0.3825 - val_acc: 0.8300\n",
      "Epoch 5/20\n",
      " - 4s - loss: 0.3865 - acc: 0.8280 - val_loss: 0.3839 - val_acc: 0.8291\n",
      "Epoch 6/20\n",
      " - 4s - loss: 0.3850 - acc: 0.8288 - val_loss: 0.3863 - val_acc: 0.8272\n",
      "Epoch 7/20\n",
      " - 4s - loss: 0.3835 - acc: 0.8285 - val_loss: 0.3843 - val_acc: 0.8278\n",
      "Epoch 8/20\n",
      " - 4s - loss: 0.3788 - acc: 0.8295 - val_loss: 0.3831 - val_acc: 0.8294\n",
      "Epoch 9/20\n",
      " - 4s - loss: 0.3793 - acc: 0.8286 - val_loss: 0.3850 - val_acc: 0.8291\n",
      "Epoch 10/20\n",
      " - 4s - loss: 0.3761 - acc: 0.8296 - val_loss: 0.3824 - val_acc: 0.8301\n",
      "Epoch 11/20\n",
      " - 4s - loss: 0.3725 - acc: 0.8315 - val_loss: 0.3959 - val_acc: 0.8266\n",
      "Epoch 12/20\n",
      " - 4s - loss: 0.3696 - acc: 0.8323 - val_loss: 0.3908 - val_acc: 0.8274\n",
      "Epoch 13/20\n",
      " - 4s - loss: 0.3689 - acc: 0.8325 - val_loss: 0.3892 - val_acc: 0.8314\n",
      "Epoch 14/20\n",
      " - 4s - loss: 0.3675 - acc: 0.8321 - val_loss: 0.3931 - val_acc: 0.8208\n",
      "Epoch 15/20\n",
      " - 4s - loss: 0.3640 - acc: 0.8325 - val_loss: 0.4002 - val_acc: 0.8261\n",
      "Epoch 16/20\n",
      " - 4s - loss: 0.3619 - acc: 0.8334 - val_loss: 0.3919 - val_acc: 0.8262\n",
      "Epoch 17/20\n",
      " - 4s - loss: 0.3626 - acc: 0.8335 - val_loss: 0.3951 - val_acc: 0.8251\n",
      "Epoch 18/20\n",
      " - 4s - loss: 0.3613 - acc: 0.8327 - val_loss: 0.4167 - val_acc: 0.8248\n",
      "Epoch 19/20\n",
      " - 4s - loss: 0.3599 - acc: 0.8342 - val_loss: 0.4002 - val_acc: 0.8262\n",
      "Epoch 20/20\n",
      " - 4s - loss: 0.3586 - acc: 0.8338 - val_loss: 0.4004 - val_acc: 0.8276\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "\n",
    "callbacks_list = [\n",
    "    ReduceLROnPlateau( \n",
    "        monitor='loss',\n",
    "        min_lr=0.001, \n",
    "        factor=0.5,\n",
    "        verbose=1,\n",
    "        patience=10) \n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit({\"summary_input\":padded_train_summaries_ids, \n",
    "                     \"other_features_input\":X_train_scaled}, \n",
    "                        y_train, \n",
    "                    epochs=20, batch_size=64, \n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=2, shuffle=True, \n",
    "                    validation_data=({\"summary_input\":padded_test_summaries_ids, \n",
    "                                      \"other_features_input\":X_test_scaled}, \n",
    "                                     y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:28:11.615471Z",
     "start_time": "2020-01-14T12:28:10.665999Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict({\"summary_input\":padded_test_summaries_ids, \n",
    "                        \"other_features_input\":X_test_scaled})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:28:13.778573Z",
     "start_time": "2020-01-14T12:28:13.740589Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:28:15.908369Z",
     "start_time": "2020-01-14T12:28:15.882381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy = 0.828, Precision = 0.828, Recall = 0.828, F1-score = 0.828'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:28:18.043359Z",
     "start_time": "2020-01-14T12:28:18.019362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5927, 1350],\n",
       "       [1161, 6130]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T09:25:32.646422Z",
     "start_time": "2020-01-14T09:25:32.606443Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 ML-GPU",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
